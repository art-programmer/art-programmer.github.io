<!DOCTYPE html>
<html>
  <head>
    <title>FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans</title>
    <link rel="stylesheet" href="css/main.css">
  </head>
  <body>
    <div class="content project_title top_block">
      <h1>FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans</h1>
    </div>

    <div class="content project_headline">
      <div class="img">
        <img class="img_teaser" src="floornet/teaser.jpg" alt="Teaser">
      </div>
      <div class="text">
        <p>Figure 1: We propose a novel network, FloorNet, to turn RGBD videos
        of indoor spaces into vector-graphics floorplans. FloorNet consists of
        three DNN branches. The first branch uses PointNet to directly consume 3D information. The second branch takes a top-down point density image in a floorplan domain with a fully convolutional network, and produces pixel-wise geometry and semantics information. The third branch produces deep image features by a dilated residual network trained on the semantic segmentation task as well as a stacked hourglass CNN trained on the room layout estimation. The PointNet branch and the floorplan branch exchanges intermediate features at every layer, while the image branch contributes deep image features into the decoding part of the floorplan branch. This hybrid DNN architecture effectively processes an input RGBD video with camera poses, covering a large 3D space.</p>
      </div>
      <hr />
    </div>

    <div class="content">
      <div class="text">
        <h3>Abstract</h3>
        <p>The ultimate goal of this indoor mapping research is to automatically reconstruct a floorplan simply by walking through a house with a smartphone in a pocket. This paper tackles this problem by proposing FloorNet, a novel deep neural architecture. The challenge lies in the processing of RGBD streams spanning a large 3D space. FloorNet effectively processes the data through three neural network branches: 1) PointNet with 3D points, exploiting the 3D information; 2) CNN with a 2D point density image in a top-down view, enhancing the local spatial reasoning; and 3) CNN with RGB images, utilizing the full image information. FloorNet exchanges intermediate features across the branches to exploit the best of all the architectures. We have created a benchmark for floorplan reconstruction by acquiring RGBD video streams for 155 residential houses or apartments with Google Tango phones and annotating complete floorplan information. Our qualitative and quantitative evaluations demonstrate that the fusion of three branches effectively improves the reconstruction quality. We hope that the paper together with the benchmark will be an important step towards solving a challenging vector-graphics reconstruction problem.</p>
      </div>
      <hr />
    </div>

    <div class="content">
      <div class="text">
        <h3>Video</h3>
      </div>
      <div class"project_video">
        <iframe width="720" height="405" src="https://www.youtube.com/embed/i7H7riAAYEc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      <hr />
    </div>

    <div class="content">
      <div class="text">
        <h3>Paper</h3>
        <div class="paper_title">FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans</div>
        <div class="paper_authors">
          <a class="author" href="http://www.cse.wustl.edu/~chenliu/"><b>Chen Liu</b><sup>*</sup></a>,
	  <a class="author">Jiaye Wu<sup>*</sup></a>,
          <a class="author" href="http://www.cs.sfu.ca/~furukawa/">Yasutaka Furukawa</a>
	  <br />
          (* indicates equal contribution.)
        </div>
	<div class="paper_venue"></div>
	<!--div class="stress"><b>spotlignt presentation</b></div-->
	<span class="link"><a href="floornet/paper.pdf">[Arxiv]</a></span>
	<span class="link"><a href="floornet/supplementary.pdf">[Supp.]</a></span>
	<span class="link"><a href="https://github.com/art-programmer/FloorNet">[Code
	and Data]</a></span>
      </div>
      <hr />
    </div>
  </body>
</html>
